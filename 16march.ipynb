{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5adf8c-bc1f-4a42-ad45-d7797dcf1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#verfitting and Underfitting are the two main problems that occur in machine learning and degrade the performance of the machine learning models.\n",
    "\n",
    "#The main goal of each machine learning model is to generalize well. Here generalization defines the ability of an ML model to provide a suitable output by adapting the given set of unknown input. It means after providing training on the dataset, it can produce reliable and accurate output. Hence, the underfitting and overfitting are the two terms that need to be checked for the performance of the model and whether the model is generalizing well or not.\n",
    "\n",
    "#Before understanding the overfitting and underfitting, let's understand some basic term that will help to understand this topic well:\n",
    "\n",
    "#Signal: It refers to the true underlying pattern of the data that helps the machine learning model to learn from the data.\n",
    "#Noise: Noise is unnecessary and irrelevant data that reduces the performance of the model.\n",
    "#Bias: Bias is a prediction error that is introduced in the model due to oversimplifying the machine learning algorithms. Or it is the difference between the predicted values and the actual values.\n",
    "#Variance: If the machine learning model performs well with the training dataset, but does not perform well with the test dataset, then variance occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af197d2-f78b-4163-8aed-aaa4df3205d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 by using some solution \n",
    "# cross-validation, train with more data , remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c6e136-e410-4a51-8a61-cf982129b0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\\n\\nNote: The underfitting model has High bias and low variance.\\n\\nReasons for Underfitting\\nThe model is too simple, So it may be not capable to represent the complexities in the data.\\nThe input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\\nThe size of the training dataset used is not enough.\\nExcessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\\nFeatures are not scaled.\\nTechniques to Reduce Underfitting\\nIncrease model complexity.\\nIncrease the number of features, performing feature engineering.\\nRemove noise from the data.\\nIncrease the number of epochs or increase the duration of training to get better results.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "'''A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\n",
    "\n",
    "Note: The underfitting model has High bias and low variance.\n",
    "\n",
    "Reasons for Underfitting\n",
    "The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "The size of the training dataset used is not enough.\n",
    "Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "Features are not scaled.\n",
    "Techniques to Reduce Underfitting\n",
    "Increase model complexity.\n",
    "Increase the number of features, performing feature engineering.\n",
    "Remove noise from the data.\n",
    "Increase the number of epochs or increase the duration of training to get better results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab08db2c-6a90-4b11-b99b-eeba1f1fa7d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4239803137.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    '''In supervised machine learning an algorithm learns a model from training data.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    " '''In supervised machine learning an algorithm learns a model from training data.\n",
    "\n",
    "The goal of any supervised machine learning algorithm is to best estimate the mapping function (f) for the output variable (Y) given the input data (X). The mapping function is often called the target function because it is the function that a given supervised machine learning algorithm aims to approximate.\n",
    "\n",
    "The prediction error for any machine learning algorithm can be broken down into three parts:\n",
    "\n",
    "Bias Error\n",
    "Variance Error\n",
    "Irreducible Error\n",
    "The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.\n",
    "\n",
    "In this post, we will focus on the two parts we can influence with our machine learning algorithms. The bias error and the variance error.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5760fb4-86fd-4ff3-8cc9-81ecad439de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bias: Bias measures the difference between the model’s prediction and the target value. If the model is oversimplified, then the predicted value would be far from the ground truth resulting in more bias.\\nVariance: Variance is the measure of the inconsistency of different predictions over varied datasets. If the model’s performance is tested on different datasets, the closer the prediction, the lesser the variance. Higher variance is an indication of overfitting in which the model loses the ability to generalize.\\nBias-variance tradeoff: A simple linear model is expected to have a high bias and low variance due to less complexity of the model and fewer trainable parameters. On the other hand, complex non-linear models tend to observe an opposite behavior. In an ideal scenario, the model would have an optimal balance of bias and variance.\\nModel generalization: Model generalization means how well the model is trained to extract useful data patterns and classify unseen data samples. \\nFeature selection: It involves selecting a subset of features from all the extracted features that contribute most towards the model performance. Including all the features unnecessarily increases the model complexity and redundant features can significantly increase the training time.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5\n",
    "'''Bias: Bias measures the difference between the model’s prediction and the target value. If the model is oversimplified, then the predicted value would be far from the ground truth resulting in more bias.\n",
    "Variance: Variance is the measure of the inconsistency of different predictions over varied datasets. If the model’s performance is tested on different datasets, the closer the prediction, the lesser the variance. Higher variance is an indication of overfitting in which the model loses the ability to generalize.\n",
    "Bias-variance tradeoff: A simple linear model is expected to have a high bias and low variance due to less complexity of the model and fewer trainable parameters. On the other hand, complex non-linear models tend to observe an opposite behavior. In an ideal scenario, the model would have an optimal balance of bias and variance.\n",
    "Model generalization: Model generalization means how well the model is trained to extract useful data patterns and classify unseen data samples. \n",
    "Feature selection: It involves selecting a subset of features from all the extracted features that contribute most towards the model performance. Including all the features unnecessarily increases the model complexity and redundant features can significantly increase the training time.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407486a6-7022-4557-9fb1-1f7be0d7ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With any model, specific features are used to determine a given outcome. If there are not enough predictive features present, then more features or features with greater importance, should be introduced. For example, in a neural network, you might add more hidden neurons or in a random forest, you may add more trees. This process will inject more complexity into the model, yielding better training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fddee3d7-161d-46cf-b913-b1a538c3eee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n general, a machine learning model analyses the data, find patterns in it and make predictions. While training, the model learns these patterns in the dataset and applies them to test data for prediction. While making predictions, a difference occurs between prediction values made by the model and actual values/expected values, and this difference is known as bias errors or Errors due to bias. It can be defined as an inability of machine learning algorithms such as Linear Regression to capture the true relationship between the data points. Each algorithm begins with some amount of bias because bias occurs from assumptions in the model, which makes the target function simple to learn. A model has either:\\n\\nLow Bias: A low bias model will make fewer assumptions about the form of the target function.\\nHigh Bias: A model with a high bias makes more assumptions, and the model becomes unable to capture the important features of our dataset. A high bias model also cannot perform well on new data.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6\n",
    "'''n general, a machine learning model analyses the data, find patterns in it and make predictions. While training, the model learns these patterns in the dataset and applies them to test data for prediction. While making predictions, a difference occurs between prediction values made by the model and actual values/expected values, and this difference is known as bias errors or Errors due to bias. It can be defined as an inability of machine learning algorithms such as Linear Regression to capture the true relationship between the data points. Each algorithm begins with some amount of bias because bias occurs from assumptions in the model, which makes the target function simple to learn. A model has either:\n",
    "\n",
    "Low Bias: A low bias model will make fewer assumptions about the form of the target function.\n",
    "High Bias: A model with a high bias makes more assumptions, and the model becomes unable to capture the important features of our dataset. A high bias model also cannot perform well on new data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c769f3c-9635-4084-8f84-3892235e6537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The variance would specify the amount of variation in the prediction if the different training data was used. In simple words, variance tells that how much a random variable is different from its expected value. Ideally, a model should not vary too much from one training dataset to another, which means the algorithm should be good in understanding the hidden mapping between inputs and output variables. Variance errors are either of low variance or high variance.\\n\\nLow variance means there is a small variation in the prediction of the target function with changes in the training data set. At the same time, High variance shows a large variation in the prediction of the target function with changes in the training dataset.\\n\\nA model that shows high variance learns a lot and perform well with the training dataset, and does not generalize well with the unseen dataset. As a result, such a model gives good results with the training dataset but shows high error rates on the test dataset.\\n\\nSince, with high variance, the model learns too much from the dataset, it leads to overfitting of the model. A model with high variance has the below problems:\\n\\nA high variance model leads to overfitting.\\nIncrease model complexities.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The variance would specify the amount of variation in the prediction if the different training data was used. In simple words, variance tells that how much a random variable is different from its expected value. Ideally, a model should not vary too much from one training dataset to another, which means the algorithm should be good in understanding the hidden mapping between inputs and output variables. Variance errors are either of low variance or high variance.\n",
    "\n",
    "Low variance means there is a small variation in the prediction of the target function with changes in the training data set. At the same time, High variance shows a large variation in the prediction of the target function with changes in the training dataset.\n",
    "\n",
    "A model that shows high variance learns a lot and perform well with the training dataset, and does not generalize well with the unseen dataset. As a result, such a model gives good results with the training dataset but shows high error rates on the test dataset.\n",
    "\n",
    "Since, with high variance, the model learns too much from the dataset, it leads to overfitting of the model. A model with high variance has the below problems:\n",
    "\n",
    "A high variance model leads to overfitting.\n",
    "Increase model complexities.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1486a-0c74-4b4b-85f8-b0e9caaf0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
